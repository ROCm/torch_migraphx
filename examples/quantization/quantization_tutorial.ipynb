{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Quantized ResNet50 via MIGraphX\n",
    "This notebook walks through the PTQ workflow for running a quantized model using torch_migraphx."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Use PyTorch's Quantization API to perform quantization\n",
    "We will closely follow the steps provided in [PyTorch docs](https://pytorch.org/docs/stable/quantization.html#prototype-fx-graph-mode-quantization) for FX quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.ao.quantization.quantize_fx as quantize_fx\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1).eval()\n",
    "input_fp32 = torch.randn(2, 3, 28, 28)\n",
    "\n",
    "torch_fp32_out = model_fp32(input_fp32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Quantization API to prepare and calibrate the model\n",
    "Torch-MIGraphX provides supported qconfig and backend configs that are the recommended settings for performing quantization that is compatible with MIGraphX. Additional configs will also work as long as the configs ensure symmetric quantization. Currently, only symmetric quantization is supported in MIGraphX.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_migraphx.fx.quantization import (\n",
    "    get_migraphx_backend_config,\n",
    "    get_migraphx_qconfig_mapping,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare\n",
    "qconfig_mapping = get_migraphx_qconfig_mapping()\n",
    "backend_config = get_migraphx_backend_config()\n",
    "\n",
    "model_prepared = quantize_fx.prepare_fx(\n",
    "    model_fp32,\n",
    "    qconfig_mapping,\n",
    "    (input_fp32, ),\n",
    "    backend_config=backend_config,\n",
    ")\n",
    "\n",
    "# Pseudo-calibrate with fake data\n",
    "for _ in range(100):\n",
    "    inp = torch.randn(2, 3, 28, 28)\n",
    "    model_prepared(inp)\n",
    "    \n",
    "\n",
    "# Convert to quantized model\n",
    "model_quantized = quantize_fx.convert_fx(\n",
    "    model_prepared,\n",
    "    qconfig_mapping=qconfig_mapping,\n",
    "    backend_config=backend_config,\n",
    ")\n",
    "\n",
    "# Reference torch int8 cpu output\n",
    "torch_qout = model_quantized(input_fp32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lower Quantized Model to MIGraphX\n",
    "This step is the same as lowering any other model using the FX Tracing path! Note that in general we need to suppress accuracy check when lowering. This is because the lowering pass will try to compare the pytorch INT8 implementation result with the MIGraphX INT8 result and in practice the different implementations can lead to significant differences for some values. In this resnet example, and most other examples, the MIGraphX implementation tends to provide better results when compared to the FP32 reference output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_migraphx.fx import lower_to_mgx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgx_model = lower_to_mgx(\n",
    "    model_quantized,\n",
    "    (input_fp32, ),\n",
    "    suppress_accuracy_check=True,\n",
    ")\n",
    "\n",
    "# MIGraphX int8 output\n",
    "mgx_out = mgx_model(input_fp32.cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PyTorch FP32 (Gold Value):\\n{torch_fp32_out}\")\n",
    "print(f\"PyTorch INT8 (CPU Impl):\\n{torch_qout}\")\n",
    "print(f\"MIGraphX INT8:\\n{mgx_out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance\n",
    "Let's do a quick test to measure the performance gain from using quantization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from torch_migraphx.fx.utils import LowerPrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this function to benchmark all modules:\n",
    "def benchmark_module(model, inputs, iterations=100):\n",
    "    model(*inputs)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    start_event.record()\n",
    "    for _ in range(iterations):\n",
    "        model(*inputs)\n",
    "    end_event.record()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    return start_event.elapsed_time(end_event) / iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark Torch FP32 for baseline\n",
    "torch_fp32_time = benchmark_module(model_fp32.cuda(), [input_fp32.cuda()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark MIGraphX FP32\n",
    "mgx_module_fp32 = lower_to_mgx(copy.deepcopy(model_fp32), [input_fp32])\n",
    "mgx_fp32_time = benchmark_module(mgx_module_fp32, [input_fp32.cuda()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark MIGraphX FP16\n",
    "mgx_module_fp16 = lower_to_mgx(copy.deepcopy(model_fp32), [input_fp32], lower_precision=LowerPrecision.FP16)\n",
    "mgx_fp16_time = benchmark_module(mgx_module_fp16, [input_fp32.cuda()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark MIGraphX INT8\n",
    "mgx_int8_time = benchmark_module(mgx_model, [input_fp32.cuda()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{torch_fp32_time=:0.4f}ms\")\n",
    "print(f\"{mgx_fp32_time=:0.4f}ms\")\n",
    "print(f\"{mgx_fp16_time=:0.4f}ms\")\n",
    "print(f\"{mgx_int8_time=:0.4f}ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
