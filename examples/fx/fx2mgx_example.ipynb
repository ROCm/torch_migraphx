{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIGraphX Accelerator Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "libpath = os.path.join(Path.cwd().parents[1],'py')\n",
    "sys.path.append(libpath)\n",
    "sys.path.append('/opt/rocm/lib')\n",
    "\n",
    "\n",
    "import torch\n",
    "import migraphx\n",
    "import torchvision.models as models\n",
    "from torch_migraphx.fx.fx2mgx import MGXInterpreter\n",
    "from torch_migraphx.fx.mgx_module import MGXModule\n",
    "import torch_migraphx.fx.tracer.acc_tracer.acc_tracer as acc_tracer\n",
    "from torch_migraphx.fx.tracer.acc_tracer.acc_shape_prop import AccShapeProp\n",
    "from torch_migraphx.fx.tools.mgx_splitter import MGXSplitter\n",
    "\n",
    "from torch.fft import fft2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a simple network we will lower to migraphx. It also contains an unsupported operation (fft2) we must handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self, k, in_ch):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(in_ch, in_ch * 2, k, padding='same')\n",
    "        self.bn = torch.nn.BatchNorm2d(in_ch * 2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear = torch.nn.Linear(224 * 224 * in_ch * 2, 64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = fft2(x).abs()  #unsupported op\n",
    "        x = x.flatten(1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k, in_ch = 3, 3\n",
    "model = ConvNet(k, in_ch)\n",
    "model.eval()\n",
    "sample_inputs = [torch.randn(50, 3, 224, 224)]\n",
    "\n",
    "model = model.cuda()\n",
    "sample_inputs = [i.cuda() for i in sample_inputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we use our custom fx tracer (acc_tracer) to generate a graph representation of the above module. The custom tracer also normalizes all supported torch operations to map to acc ops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trace model using custom tracer\n",
    "traced = acc_tracer.trace(model, sample_inputs)\n",
    "traced.graph.print_tabular()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we split the graph into subgraphs that are supported by migraphx and ones that need to run via the torch implementation. Submodules named 'run_on_acc_{}' are marked to be lowered to migraphx and the ones named 'run_via_torch_{}' are marked to be executed though its original torch implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = MGXSplitter(traced, sample_inputs)\n",
    "_ = splitter.node_support_preview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_mod = splitter()\n",
    "print(split_mod.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(split_mod._run_on_acc_0.graph)\n",
    "print(split_mod._run_via_torch_1.graph)\n",
    "print(split_mod._run_on_acc_2.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we convert any submodules that are eligible to be lowered to migraphx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need sample inputs when lowering submodules\n",
    "def get_submod_inputs(mod, submod, inputs):\n",
    "    acc_inputs = None\n",
    "\n",
    "    def get_input(self, inputs):\n",
    "        nonlocal acc_inputs\n",
    "        acc_inputs = inputs\n",
    "\n",
    "    handle = submod.register_forward_pre_hook(get_input)\n",
    "    mod(*inputs)\n",
    "    handle.remove()\n",
    "    return acc_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, _ in split_mod.named_children():\n",
    "    if \"_run_on_acc\" in name:\n",
    "        submod = getattr(split_mod, name)\n",
    "        # Get submodule inputs for fx2trt\n",
    "        acc_inputs = get_submod_inputs(split_mod, submod, sample_inputs)\n",
    "        AccShapeProp(submod).propagate(*acc_inputs)\n",
    "\n",
    "        # fx2trt replacement\n",
    "        interp = MGXInterpreter(\n",
    "            submod,\n",
    "            acc_inputs\n",
    "        )\n",
    "        interp.run()\n",
    "        mgx_mod = MGXModule(interp.program, interp.get_input_names())\n",
    "\n",
    "        setattr(split_mod, name, mgx_mod)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The creation of MGXModule automatically runs all optimization passes available in MIGraphX and stores the complied program. We can see the hip instructions by printing the stored programs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_mod._run_on_acc_0.program.print()\n",
    "split_mod._run_on_acc_2.program.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we ensure that the converted modules produce the same output as the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_mod = split_mod.cuda()\n",
    "model = model.cuda()\n",
    "sample_inputs = [i.cuda() for i in sample_inputs]\n",
    "\n",
    "torch_out = model(*sample_inputs)\n",
    "lowered_model_out = split_mod(*sample_inputs)\n",
    "\n",
    "torch.testing.assert_close(torch_out,\n",
    "                            lowered_model_out,\n",
    "                            atol=3e-3,\n",
    "                            rtol=1e-2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modules that contain MGXModules as submodules can be saved and loaded in the same manner as torch modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(split_mod, 'split_mod.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_split_mod = torch.load('split_mod.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_mod_out = reload_split_mod(*sample_inputs)\n",
    "\n",
    "torch.testing.assert_close(torch_out,\n",
    "                            reload_mod_out,\n",
    "                            atol=3e-3,\n",
    "                            rtol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_migraphx.fx import lower_to_mgx\n",
    "lowered_model = lower_to_mgx(model, sample_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowered_out = lowered_model(*sample_inputs)\n",
    "torch_out = model(*sample_inputs)\n",
    "torch.testing.assert_close(torch_out,\n",
    "                            lowered_out,\n",
    "                            atol=3e-3,\n",
    "                            rtol=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
